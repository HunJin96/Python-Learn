{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchtext.vocab as torchvocab\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import gensim\n",
    "import time\n",
    "import random\n",
    "import snowballstemmer\n",
    "import collections\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"Corpus_190905_simplify-final.xlsx\")\n",
    "data1 = pd.read_excel(\"Corpus_190905_simplify-final-1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    ## Stemming\n",
    "    text = text.split()\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    stemmed_words = [stemmer.stemWord(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = []\n",
    "for review in data[\"Description\"].astype(str):\n",
    "    train_tokenized.append(clean_text(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized = []\n",
    "for reviews in data1[\"Description\"].astype(str):\n",
    "    test_tokenized.append(clean_text(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized1=[]\n",
    "train_tokenized1=[]\n",
    "for i in range (len(train_tokenized)):\n",
    "    train_tokenized1.append(train_tokenized[i].split())\n",
    "for i in range (len(test_tokenized)):\n",
    "    test_tokenized1.append(test_tokenized[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(chain(*train_tokenized1))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvmodel = gensim.models.KeyedVectors.load_word2vec_format('gensim_WordVec.txt',\n",
    "                                                          binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx  = {word: i+1 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<unk>'] = 0\n",
    "idx_to_word = {i+1: word for i, word in enumerate(vocab)}\n",
    "idx_to_word[0] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_samples(tokenized_samples, vocab):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in word_to_idx:\n",
    "                feature.append(word_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def pad_samples(features, maxlen=60, PAD=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) >= maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            while(len(padded_feature) < maxlen):\n",
    "                padded_feature.append(PAD)\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(pad_samples(encode_samples(train_tokenized1, vocab)))\n",
    "train_labels =torch.tensor([score for score in data[\"Pin Type -Symbol\"]])\n",
    "test_features = torch.tensor(pad_samples(encode_samples(test_tokenized1, vocab)))\n",
    "test_labels =torch.tensor([score for score in data1[\"Pin Type -Symbol\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2439, 2455, 3079,  ...,    0,    0,    0],\n",
       "        [1589,  467, 2554,  ...,    0,    0,    0],\n",
       "        [2455, 1708,  583,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [1797, 1708, 2294,  ...,    0,    0,    0],\n",
       "        [ 784, 1708, 2294,  ...,    0,    0,    0],\n",
       "        [2039,  646,  567,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3368, 3079,  567,  ...,    0,    0,    0],\n",
       "        [2595,  982, 2589,  ...,    0,    0,    0],\n",
       "        [2518, 1708, 2294,  ..., 1566, 3574, 2439],\n",
       "        ...,\n",
       "        [2890, 3079, 3553,  ...,    0,    0,    0],\n",
       "        [1017,  646, 2554,  ...,    0,    0,    0],\n",
       "        [1301, 3079, 1662,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 bidirectional, weight, labels, use_gpu, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.use_gpu = use_gpu\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,\n",
    "                               num_layers=num_layers, bidirectional=self.bidirectional,\n",
    "                               dropout=0.4)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.Linear(num_hiddens * 4, labels)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        states, hidden = self.encoder(embeddings.permute([1, 0, 2]))\n",
    "        encoding = torch.cat([states[0], states[-1]], dim=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.zeros(vocab_size+1, embed_size)\n",
    "\n",
    "for i in range(len(wvmodel.index2word)):\n",
    "    try:\n",
    "        index = word_to_idx[wvmodel.index2word[i]]\n",
    "    except:\n",
    "        continue\n",
    "    weight[index, :] = torch.from_numpy(wvmodel.get_vector(\n",
    "        idx_to_word[word_to_idx[wvmodel.index2word[i]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "embed_size = 100\n",
    "num_hiddens = 100\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 7\n",
    "lr = 0.8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_gpu = True\n",
    "net = SentimentNet(vocab_size=(vocab_size+1), embed_size=embed_size,\n",
    "                   num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                   bidirectional=bidirectional, weight=weight,\n",
    "                   labels=labels, use_gpu=use_gpu)\n",
    "net.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_set = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 1.4363, train acc: 0.46, test loss: 0.9746, test acc: 0.65, time: 2.44\n",
      "epoch: 1, train loss: 0.9520, train acc: 0.68, test loss: 0.8006, test acc: 0.74, time: 2.40\n",
      "epoch: 2, train loss: 0.7395, train acc: 0.76, test loss: 0.8273, test acc: 0.71, time: 2.39\n",
      "epoch: 3, train loss: 0.6309, train acc: 0.79, test loss: 0.7611, test acc: 0.75, time: 2.40\n",
      "epoch: 4, train loss: 0.5558, train acc: 0.82, test loss: 0.9065, test acc: 0.71, time: 2.39\n",
      "epoch: 5, train loss: 0.5081, train acc: 0.84, test loss: 0.8586, test acc: 0.74, time: 2.39\n",
      "epoch: 6, train loss: 0.4660, train acc: 0.85, test loss: 0.7918, test acc: 0.77, time: 2.40\n",
      "epoch: 7, train loss: 0.4271, train acc: 0.86, test loss: 0.8277, test acc: 0.75, time: 2.38\n",
      "epoch: 8, train loss: 0.3928, train acc: 0.88, test loss: 0.9555, test acc: 0.74, time: 2.40\n",
      "epoch: 9, train loss: 0.3702, train acc: 0.88, test loss: 0.8987, test acc: 0.76, time: 2.40\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss, test_losses = 0, 0\n",
    "    train_acc, test_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "    c = []\n",
    "    b = []\n",
    "    for feature, label in train_iter:\n",
    "        n += 1\n",
    "        net.zero_grad()\n",
    "        feature = Variable(feature.cuda())\n",
    "        label = Variable(label.cuda())\n",
    "        score = net(feature)\n",
    "        loss = loss_function(score, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                 dim=1), label.cpu())\n",
    "        train_loss += loss\n",
    "    with torch.no_grad():\n",
    "        for test_feature, test_label in test_iter:\n",
    "            m += 1\n",
    "            test_feature = test_feature.cuda()\n",
    "            test_label = test_label.cuda()\n",
    "            test_score = net(test_feature)\n",
    "            test_loss = loss_function(test_score, test_label)\n",
    "            test_acc += accuracy_score(torch.argmax(test_score.cpu().data,\n",
    "                                                    dim=1), test_label.cpu())\n",
    "            c.append(test_label.cpu())\n",
    "            b.append(torch.argmax(test_score.cpu().data,\n",
    "                                                    dim=1))\n",
    "            test_losses += test_loss\n",
    "        \n",
    "    end = time.time()\n",
    "    runtime = end - start\n",
    "    print('epoch: %d, train loss: %.4f, train acc: %.2f, test loss: %.4f, test acc: %.2f, time: %.2f' %\n",
    "          (epoch, train_loss.data / n, train_acc / n, test_losses.data / m, test_acc / m, runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "ab=''\n",
    "a.append(c)\n",
    "new_crazy = filter(str.isdigit, str(a))\n",
    "asd=','\n",
    "ab = asd.join((list(new_crazy)))\n",
    "First = ab.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "e =''\n",
    "d.append(b)\n",
    "new_crazy1 = filter(str.isdigit, str(d))\n",
    "asdf=','\n",
    "bf = asdf.join((list(new_crazy1)))\n",
    "bf\n",
    "MOdelTrain = bf.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 305,   30,   65,    5,   43,   19,    0],\n",
       "       [  21, 1099,   62,    4,  142,   26,    1],\n",
       "       [  25,  104,  967,    0,   91,   47,    1],\n",
       "       [  11,    6,   40,  172,   23,    2,    1],\n",
       "       [  48,  113,   78,   12, 1275,   10,    3],\n",
       "       [  10,   29,   29,    0,   26,   24,    3],\n",
       "       [   0,    5,    0,    0,    4,    2,   18]], dtype=int64)"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(First, MOdelTrain,labels=[\"0\", \"1\",\"2\",\"3\",\"4\", \"5\",\"6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.599681371202549"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 1617+2849+2668+1545+1058+537+642\n",
    "d = 18203\n",
    "acc = c/d\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
